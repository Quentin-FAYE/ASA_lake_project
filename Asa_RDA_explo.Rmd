---
title: "Asa_data_explo_RDA"
author: "QuentinFAYE"
date: "2025-11-10"
output:
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Data contexte 

Data are collected on the long term on one lake (see description below). Nutriments (phosphorus, nitrate, biological oxygen demand) and biodiversity (phytoplanckton, primary production, fishes) are monitored each year.

## Data origine 
Journal of Applied Ecology 2014, 51, 560–571
 doi: 10.1111/1365-2664.12245
 The relevance of ecological status to ecosystem
 functions and services in a large boreal lake
 Kimmo T. Tolonen1*, Heikki H€ am€ al€ ainen1, Anssi Lensu1, Jarmo J. Meril€ ainen1,
 Arja Palom€ aki2 and Juha Karjalainen1
 1Department of Biological and Environmental Science, University of Jyv€askyl€a, P.O.Box 35, Jyv€askyl€a FI-40014,
 Finland; and 2Institute for Environmental Research, University of Jyv€askyl€a, P.O.Box 35, Jyv€askyl€a FI-40014, Finland


## Packages needed + data loading

```{r}
library(ade4)
library(vegan)
library("ggplot2")
library("factoextra")
library(corrplot)
library(RVAideMemoire)
library("PerformanceAnalytics")
#library(GGally)
library(MASS)
library("xlsx")
library(dplyr)
```

## + X data loading

```{r}
nutriment=read.xlsx(file="data_lake.xlsx", sheetIndex = 2,  )
colnames(nutriment)=c("Year","Area","P_MWL","P_Ind_L", "P_O_L","N_MWL","N_Ind_L", "N_O_L","BOD_MWL","BOD_Ind_L", "BOD_O_L")
nutriment=nutriment[-1,] # les deux premières lignes sont les unites + les headers, on renommes les header on doit donc s'assurer que il y'ai que les valeurs de nos variables
nutriment$Area=as.factor(nutriment$Area) # les deux premières VA sont des categories
nutriment$Year=as.factor(nutriment$Year)

# "BOD_Other loading" a une seul valeur -> on surpprime la colonne 
nutriment=nutriment[,-11]

# donnes ne sont pas numerique ? mais des "character"
class(nutriment$P_MWL)

# on doit tout changer en numérique + noramliser 

nutriment$P_MWL=gsub(',','.',nutriment$P_MWL)
nutriment$P_MWL=scale(as.numeric(nutriment$P_MWL))

nutriment$P_Ind_L=gsub(',','.',nutriment$P_Ind_L)
nutriment$P_Ind_L=scale(as.numeric(nutriment$P_Ind_L))

nutriment$P_O_L=gsub(',','.',nutriment$P_O_L)
nutriment$P_O_L=scale(as.numeric(nutriment$P_O_L))

nutriment$N_MWL=gsub(',','.',nutriment$N_MWL)
nutriment$N_MWL=scale(as.numeric(nutriment$N_MWL))

nutriment$N_Ind_L=gsub(',','.',nutriment$N_Ind_L)
nutriment$N_Ind_L=scale(as.numeric(nutriment$N_Ind_L))

nutriment$N_O_L=gsub(',','.',nutriment$N_O_L)
nutriment$N_O_L=scale(as.numeric(nutriment$N_O_L))

nutriment$BOD_MWL=gsub(',','.',nutriment$BOD_MWL)
nutriment$BOD_MWL=scale(as.numeric(nutriment$BOD_MWL))

nutriment$BOD_Ind_L=gsub(',','.',nutriment$BOD_Ind_L)
nutriment$BOD_Ind_L=scale(as.numeric(nutriment$BOD_Ind_L))

summary(nutriment$P_MWL)

```

#### reflexion sur les données manquante des Va X
------------brouillon -----------------------------
# BCP de donnes manquante ? qu'est ce qu' on fait ? 
# 1- on eleves les lignes avec des donnes manquantes
# 2  in tire dans dans une distribtuoion simmilaire pour completer les donees 
#--> deux options sont valides

hist(nutriment[2:52,6])
hist(nutriment[53:96,6]) # pour le deuxième cites valeur globlment les meme -> on peut faire hypothèse que pour le premier site aussi 

plot(y=nutriment[nutriment$Area=="Central Paijanne",6] , x=seq(1968, 2011)) # pas de tendance 

plot(y=nutriment[nutriment$Area=="Northen Paijanne",6] , x=seq(1960, 2011)) # clair tendance -> on peut faire regression lineraire our obtenir valeur manuante + ajouter aletoir avec la variabilité 


#hist( a[ !a==0 ])



le P nutriment est en Kg P / day et les autres sont en tonnes ? ->normaliser
les donnes qui manque vont de l'annee 1960-1974  , ces données sont des X que l'on veut utilisé pour Y ( que Y soit phyto ou fish)  or les donnes de nos Y commence à l'annee 1975. C'etais donc un test


--> toute donnée qui manque sont  avant 1975  et donc ca nous intersse pas de les completer car les donnes de bio/ espcèes commence à partri de 195=75
----------------------------------------------------------------




## VA X pre-traitement suppresion des lignes avec des data manquantes
```{r}
nutriment2=nutriment[16:95,] # Options 1 on ne conserver que les lignes ou les nutirments sont dispos : enlèves donnes avant 1975


nutriment2=nutriment2[-37,] # supprime la ligne de l'année 2011 car donnees manquante 
# bourillon pour cette étpa---
#nutriment2=nutriment2[-nutriment2$year==2011,]
#nutriment2$year==2011 identifier quelle lignes il y a en plus des données---
```

## load Y Va 
```{r}
bio=read.xlsx(file="data_lake.xlsx", sheetIndex = 3, header=T)
bio=bio[-c(seq(118,134)),-c(9,8,7)] # enlève les lignes et colones vides

# redefinis informatiqument les donnees
bio$Phytoplankton.richness=as.numeric(bio$Phytoplankton.richness)
bio$Primary.production..g.C.m.2.=as.numeric(bio$Primary.production..g.C.m.2.)
bio$Fish.catch..kg.gill.net.1.day.1.=as.numeric(bio$Fish.catch..kg.gill.net.1.day.1.)


```
### remplire les annees qui manque
# hypothèses , justification : si un case vide on perds l'informations de la lignes , donc on perd un indivs statistiquz , si une case Na on perd l'info su les autres cases de la lignes 

# on souhaite donc remplir le tableau sans rajouter trop d 'information  
# on regarde l evoluation des ses variables en foncttion du temps et on fait un regression en fonction du temps + bruit gaussion et on complète le tableau comme cela
# pk on peut se mettre de faie cela, car on exploita pas la relations  entres les variables et le temps dans la RDA donc si on inferes les valeurs menquante en fonction du temps , on ajoute bien des infromations qu'une seul fois,  rapport a si on predisait les valeurs manquat à partir d 'une variable utilis" dans la RDA,  cela creerais un lien artificiel entres les la VA predite et la VA explicative, 

```{r}
##-----------------------fish catch pour chaque sous basin

plot(x=bio[1:23,1] , # temps
     y=bio[1:23,6] )
# -> relation linéar bof mais toujouts meilleur que prendre la mooyennes 
x=bio[1:23,1]  # temps
     y=bio[1:23,6] # fish catsh
Poro=lm(y~x)
summary(Poro) # 2 coef signficiativ -> valide le modèle 

#for which years data is missing
missing_years=c()
for (i in 1:23) {
  #print(i) # " verif"
  if (is.na( bio[i,6]))  { missing_years=c(missing_years,i)  }
  
}
#missing_years # verif

predicted_values <- predict(Poro, newdata = data.frame(x = bio[missing_years, 1])) 
bruit=  rnorm(n=length(missing_years), mean = 0, sd=sd(bio[1:23,6],na.rm = T) )
predicted_values=predicted_values+bruit

bio_filled=bio
bio_filled[missing_years,6]=predicted_values

```


```{r}
bio_moyennes <- bio %>%
  group_by(Year, Area) %>%
  summarise(
    Phytoplankton.richness = mean(`Phytoplankton.richness`, na.rm = TRUE),
    Primary.production..g.C.m.2. = mean(`Primary.production..g.C.m.2.`, na.rm = TRUE),
    Fish.catch..kg.gill.net.1.day.1. = mean(`Fish.catch..kg.gill.net.1.day.1.`, na.rm = TRUE),
    .groups = "drop"
  )
bio_moyennes<- na.omit(bio_moyennes) # retires les lignes ou il manque des valeurs


# Normalisation 
bio_moyennes$Phytoplankton.richness=scale(bio_moyennes$Phytoplankton.richness)
bio_moyennes$Primary.production..g.C.m.2.=scale(bio_moyennes$Primary.production..g.C.m.2.)
bio_moyennes$Fish.catch..kg.gill.net.1.day.1.=scale(bio_moyennes$Fish.catch..kg.gill.net.1.day.1.)

summary(bio_moyennes$Fish.catch..kg.gill.net.1.day.1.) # verification que la noramalisationa eu lieu
```

## Data exploration 

```{r}
cor(nutriment2[,3:10])
M <- cor(nutriment2[,3:10])
corrplot(M, method = "circle")

```
 correltation important  P_MWL et BOD_Mwl = on retire BOD_MWL
 correlation importante  P_O_L N_O_L = on retire P_O_L

## ne pas tourner se chunck 
##### brouilon---------------------------------------------------
bio=read.xlsx(file="data_lake.xlsx", sheetIndex = 3, header=T)
bio=bio[-c(seq(118,134)),-c(9,8,7)] # enlève les lignes et colones vides

#### redefinis informatiqument les donnees
bio$Phytoplankton.richness=as.numeric(bio$Phytoplankton.richness)
bio$Primary.production..g.C.m.2.=as.numeric(bio$Primary.production..g.C.m.2.)
bio$Fish.catch..kg.gill.net.1.day.1.=as.numeric(bio$Fish.catch..kg.gill.net.1.day.1.)



##### Supposons que ton data.frame s'appelle 'bio'
##### Voici le script pour calculer les moyennes par année et par Area

bio_moyennes <- bio %>%
  group_by(Year, Area) %>%
  summarise(
    Phytoplankton.richness = mean(`Phytoplankton.richness`, na.rm = TRUE),
    Primary.production..g.C.m.2. = mean(`Primary.production..g.C.m.2.`, na.rm = TRUE),
    Fish.catch..kg.gill.net.1.day.1. = mean(`Fish.catch..kg.gill.net.1.day.1.`, na.rm = TRUE),
    .groups = "drop"
  )

bio_moyennes<- na.omit(bio_moyennes) # retires les lignes ou il manque des valeurs

phyto_m=bio_moyennes[,1:3] # a ne pas avoir normaliser av 
phyto_m$Year=as.factor(phyto_m$Year)
phyto_m$Area=as.factor(phyto_m$Area)
##### Afficher le résultat
head(bio_moyennes)
#####nut2_bio_m=cbind(bio_moyennes,nutriment2) # fusion a partir des anees

nut2_bio_m <- merge(nutriment2,bio_moyennes, by=c("Year", "Area"), all = TRUE)

nut2_bio_m[nut2_bio_m == "NaN"] <- NA


cor(nut2_bio_m[,3:13])
M <- cor(nut2_bio_m[,3:13],  use = "pairwise.complete.obs")
corrplot(M, method = "circle")


 
forte correlation negative P_Ind_L et phytoplancton.
##  co-inertia 1 - AFC/CA and PCA expolration ne pas run le chunck----- 

#### pca ne doit pas avoir de na et peut pas utiliser des VA categorie
bio_moyennes_complet <- na.omit(bio_moyennes)
####pcasp <- dudi.pca(bio_moyennes_complet, scannf = FALSE, nf = 2) # peut pas utiliser des VA categorie
pcaenv <- dudi.pca(nutriment2[,3:10],scannf=F,nf=2) # perd le lien avec les années
####coi <- coinertia(pcaenv,coaphyto,scannf=F,nf=2) # equal row numbers is needed
nutriment3_phyto<- merge(nutriment2, phyto_m, by = c("Year", "Area"))
coaphyto <- dudi.coa(nutriment3_phyto[,c(1,2,11)],scannf=F,nf=2) 
pcaenv <- dudi.pca(nutriment3_phyto[,3:10],row.w =coaphyto$lw,scannf=F,nf=2) # perd le lien avec les années


coi <- coinertia(pcaenv,coaphyto,scannf=F,nf=2) 
coi
str(coi)
randtest(coi,nrepet=1000)
plot(randtest(coi),main="Monte-Carlo test")


summary(coi)
coi
randtest(coi,nrepet=1000)
plot(randtest(coi),main="Monte-Carlo test")


# justification de RDA et pas CCA ou pas COinertie
tableau des X -> VA continues 
tableau des Y -> VA qualite + comptage ( semie quanti) + VA continue
si on ne conserve que comptage + Va Continue on a deux tableau de VA continue/semi- quantitative
aussi nous voulons contraindre l'analyse on pense que les donnees environemtale explique les donnes des fonctionalité et de diversité de l' ecosystème 

##  RDA PCA PCA avec vegan : Respecter les chunck à ne pas runer !! 

```{r}
env_bio=merge(nutriment2,bio_moyennes,by = c("Year", "Area")) # permet d'avoir des tableau de meme taille

# verification que donnes bien centréé reduites  meme apres la selec
mean(env_bio$Phytoplankton.richness)

# RDA avec toutes les VAs
rda1=rda(env_bio[,11:13]~.,env_bio[,3:10])

# RDA avec  les VA les plus pertinantes
rda2=ordistep(rda1, perm.max=500)
rda3=rda(env_bio[,11:13]~env_bio$P_MWL+env_bio$N_MWL+env_bio$BOD_Ind_L,env_bio[,3:10])

# visualtion 
plot(rda1, scaling=2)
plot(rda3,scaling=2) 
plot(rda2,scaling=2)

# faire un code pour ne pas avoir les lignes 
```
# scaling 1 devitnion ? 
 scaling = 2: preserve correlations between descriptors. Interesting if we are interested in the relation among variables.
plot(rdadoubs,scaling=1)  # scaling = 1: preserve euclidian distances between observations

```{r}
summary(rda2)
anova.cca(rda3) #tt le modèle avec test de permutations
#anova(rda3) # meme cote

```
25% de variance des mesures de fonctionnalites et de diversite de l'ecosystme sont expliqués par la ravilaite enregsitré dans dans les variables explicative ! Dont 21% dans le premiers Axe.
L' anova sur tout le modèle montre que le modèle creer est signifactivment meilleur que le modèle Nul, des liens entres le tableaux X et Y ont été identifé signifivaticment. 
Les autres axes ne sont pas signifiactive, on ne doit pas les utiliser dans l' interpretation. 

#### validation du modèle RDA et inerpretation 
```{r}

anova(rda3, by="term", model="direct", data=env_bio[,c(3,6,10)], perm.max=10000 , step=1000) # ne valide pas interet P_MWL 

anova(rda1, by="term", model="direct", data=env_bio[,c(3,6,10)], perm.max=10000 , step=1000) # ne valide pas interet P_MWL 


anova(rda3 , by="margin") # valide tout, quelle est la diferrences
```
deux Analyse de la variances différentes une valide toutes les VA X expllicatives l'autres n'en conserve que deux 

BY TERM : anova(..., by="term") / anova(..., by="axis") (Sequential Test)

How it works: Variables are added to the model one at a time, in the order they are specified in the formula. The significance test for each term measures the additional variance it explains, given the variables that were entered before it.

BY margin : 
he significance of each variable is tested by seeing how much the model fit worsens when that specific variable is removed from the full model. It measures the unique contribution of each variable, after accounting for all other variables in the model.

INterpretation DONC 
Anova by term ->le modèle seul avec P_MWL n' pas suffisament de vairavilté mais quand on rajoute les autres VA N_MWL et BIO_INd 
anova by margin -> chaque VA est interesante dans le modèle car en enlvever une diminus signifiactivment la vairavlité contraintre expliqué par le moddèle

#### RDA validation de chaque AXE
```{r}
anova(rda3, by="axis") # ne valide que axis 1 
MVA.synt(rda3) # interpreter
```
Ne conserver que le premier Axe, ce qui portaient le maximume de contraintres expliqueé 21% les autres axes ne doivent donc pas etre utilisé pour l'interpretation. 
dans l'axe 1 on retrouve 84% de l'information contrainte capté par le modèle.

```{r}
test=vif.cca(rda2)   # inflaction de la variance ?
vif.cca(rda3) # all inferior of 10 so -> ok
test

# plot d 'exploration, pas tres interessant grafiquemr, mettre  sp et Va envir sur meme graph 
plot(rda3,scaling=2, display = "sp" )
plot(rda3, scaling=2)
plot(rda3,scaling=1, display = "sp" )
```

La VIF aassociée à chaque est bien en dessous de 5 et meme de 3 il n'y a paq de probèle de ce coté. pas 'inflation de la variaciance à cause de colinearité hypothehtique
```{r}
#s.corcircle()
```


```{r}
#scores()

coef(rda3) # pas comment sont represent ? comment faire la transformation d'axe ? 
MVA.plot(rda3,"corr",space=1)#constrained PCA interpretation ? tailles des fless pas du tout les memes et dans le plan Rd3 c'est l'inverse, bio tres proche du centre et env flèche plus grande ? 
goodness(rda3) # different des coef
```
# partial RDA avec temps ? 



## to do 
#Partition The Variation Of Community Matrix By 2, 3, Or 4 Explanatory Matrices



```{r}
# interpretation ICI 


# quelle des VA environnemtale explique le mieux la variances de la comunnaute de bio 
# regarder toutes à la fois
mod=varpart(env_bio[,11:13], env_bio$P_MWL, env_bio$N_MWL, env_bio$BOD_Ind_L)
mod # comment interpreter ? 
plot(mod, bg=2:5)

# ou regarder une par une 

rda_PWL=rda(env_bio[,11:13], env_bio$P_MWL)
rda_PWL  # comment interpreter ?  # comment on interpprète cela ??? quelle eingevalue est d 'interet ? 

```



#calculate with ADE4 the percentage of species explained by env:
sum(pcaivdoubs$eig)/sum(pcafau$eig)
### avec ADE4

```{r}
pca_bio=dudi.pca(env_bio[,11:13], scale = F , nf=2, scannf = F)
pcai_env=pcaiv(pca_bio, env_bio[,3:10], scannf = F , nf=2)

randtest(pcai_env)#with ADE4 # o-value assez elevé pas ouf
plot(randtest(pcai_env))
#modèle on significatif car on a  mis touts les VA explicative# 
# recommencer avec les VA selectionées avec VEGAN

pcai_env2=pcaiv(pca_bio,env_bio[,c(3,6,10)], scannf = F , nf=2)
randtest(pcai_env2)#with ADE4 # o-value assez elevé pas ouf
plot(randtest(pcai_env2))

# le modèle avec les bonnes VA  est singficaviemtn different d 'une modèle issue d'une mixtes randome.
```





```{r}

summary(pcai_env2)
plot(pcai_env2)
sum(pcai_env2$eig/sum(pca_bio$eig)) #  de bio expliqué par l' environnement 

```

On retrouve la meme chose aque avec les stats de Vegan , CAD 
- 25% de la variabilité
- 84% de la varaiabilitz capté du modèle est contenue dans l'axe 1 

ligne 17 semble tres importants ? trop importantes ? 
```{r}
boxplot(env_bio$Primary.production..g.C.m.2.)
# le point ici est probablement un out-layer 
# refaire l'analyse ? 


pca_bio=dudi.pca(env_bio[-17,11:13], scale = F , nf=2, scannf = F)
pcai_env2=pcaiv(pca_bio,env_bio[-17,c(3,6,10)], scannf = F , nf=2)
plot(pcai_env2)
summary(pcai_env2)

```
modèle meilleur 29% de la vairabilit' expliqué.-> tout refaire 

jsp ce que c'est ca.
```{r}
#randtest(discrimin(pca_bio, env_bio$Area, scan = FALSE), 99) # jsp pas à quoi ça sert 
```



Interpreter ? 
Decomposition per axis:
   iner inercum inerC inercumC ratio    R2 lambda
1 1.271    1.27 1.236     1.24 0.973 0.529  0.654
2 0.905    2.18 0.936     2.17 0.998 0.137  0.128


```{r}
s.corcircle(pcai_env2$cor)
s.corcircle(pcai_env2$c1)
# ce qui est interessant c'est les deux ensemble ? 

s.match(pcai_env2$ls, pcai_env2$li) # comment in entreprète cela 

```


## A faire est a interpreter ? 
$ls Ces scores de site sont ainsi des combinaisons linéaires d'abondances d'espèces maximisant la variance expliquée par les variables environnementales
Les valeurs ajustées de ces scores, prédites par les variables environnementales, sont contenues dans $li
Les résidus du modèle global espèces-environnement sont représentés par des flèches (chaque site est représenté par une flèche, le point de départ correspondant à son score environnemental ajusté et la pointe à sa composition). Une flèche courte indique une
bonne concordance entre la composition spécifique d'un site et sa prédiction par les
conditions environnementales, tandis qu'une flèche longue indique une divergence.
se sont donc les residues entres concodrande envrionemtale et composisiotn qui sont representé ??


# RDA represtation 
```{r}
# Charger la bibliothèque # Partie de code faire avec Claude Sonnet 4.5 
# car temps limité + pas bcp d 'expreience avec ggplot2 
# bidouiller ensuite manuellement pour adapter le code 
library(ggplot2)


# Exemple de données (remplacez par vos données)
# data(doubs) # Exemple de dataset ade4

#env_bio[-17,11:13], scale = F , nf=2, scannf = F)
#pcai_env2=pcaiv(pca_bio,env_bio[-17,c(3,6,10)]

env <-  env_bio[-17,c(3,6,10)]
spe <- env_bio[-17,11:13]

# Supposons que vous avez déjà fait une RDA avec pcaiv()
 rda_result <- pcai_env2 #=pcaiv(dudi.pca(spe, scannf = FALSE), env, scannf = FALSE)

# FONCTION POUR CRÉER LE TRIPLOT COMPLET
plot_rda_triplot <- function(rda_obj, axes = c(1, 2), 
                              scaling = 1,
                              arrow_col_sites = "blue",
                              arrow_col_env = "red", 
                              arrow_col_spe = "darkgreen",
                              mult_env = 1,
                              mult_spe = 1) {
  
  # Extraire les coordonnées
  # Sites (scores)
  sites_lc <- rda_obj$li[, axes]  # Valeurs prédites (contraintes)
  sites_ls <- rda_obj$ls[, axes]  # Valeurs observées
  
  # Variables environnementales (loadings)
  env_scores <- rda_obj$c1[, axes] * mult_env
  
  # Espèces (loadings)
  spe_scores <- rda_obj$co[, axes] * mult_spe
  
   # Définir les limites du graphique
  # Combiner sans tenir compte des noms de colonnes
  all_x <- c(sites_lc[, 1], sites_ls[, 1], env_scores[, 1], spe_scores[, 1])
  all_y <- c(sites_lc[, 2], sites_ls[, 2], env_scores[, 2], spe_scores[, 2])
  xlim <- range(all_x) * 1.2
  ylim <- range(all_y) * 1.2
  # Créer le graphique vide
  plot(0, 0, type = "n", 
       xlim = xlim, ylim = ylim,
       xlab = paste0("Axe ", axes[1], " (", 
                     round(rda_obj$eig[axes[1]]/sum(rda_obj$eig)*100, 1), "%)"),
       ylab = paste0("Axe ", axes[2], " (", 
                     round(rda_obj$eig[axes[2]]/sum(rda_obj$eig)*100, 1), "%)"),
       main = "Triplot RDA - Sites, Variables environnementales et Espèces",
       las = 1)
  
  # Ajouter la grille
  abline(h = 0, v = 0, lty = 2, col = "gray60")
  
  # 1) FLÈCHES SITES: valeurs observées -> valeurs prédites
  arrows(sites_ls[, 1], sites_ls[, 2],
         sites_lc[, 1], sites_lc[, 2],
         length = 0.08, col = arrow_col_sites, lwd = 1.5)
  
  # Points pour les sites observés et prédits
  points(sites_ls[, 1], sites_ls[, 2], pch = 21, 
         bg = "lightblue", col = "blue", cex = 1.2)
  points(sites_lc[, 1], sites_lc[, 2], pch = 21, 
         bg = "blue", col = "blue", cex = 1.2)
  
  # Labels des sites (optionnel)
  text(sites_lc[, 1], sites_lc[, 2], labels = rownames(sites_lc),
       pos = 3, cex = 0.6, col = "blue")
  
  # 2) FLÈCHES VARIABLES ENVIRONNEMENTALES
  arrows(0, 0, env_scores[, 1], env_scores[, 2],
         length = 0.1, col = arrow_col_env, lwd = 2.5)
  
  # Labels des variables environnementales
  text(env_scores[, 1], env_scores[, 2], 
       labels = rownames(env_scores),
       pos = 4, cex = 0.8, col = arrow_col_env, font = 2)
  
  # 3) FLÈCHES ESPÈCES/VARIABLES BIOLOGIQUES
  arrows(0, 0, spe_scores[, 1], spe_scores[, 2],
         length = 0.08, col = arrow_col_spe, lwd = 1.5)
  
  # Labels des espèces
  text(spe_scores[, 1], spe_scores[, 2], 
       labels = rownames(spe_scores),
       pos = 4, cex = 0.7, col = arrow_col_spe)
  
  # Légende
  legend("bottomleft", 
         legend = c("Sites observés", "Sites prédits", 
                    "Var. environnementales", "Espèces"),
         col = c("blue", "blue", arrow_col_env, arrow_col_spe),
         pch = c(21, 21, NA, NA),
         pt.bg = c("lightblue", "blue", NA, NA),
         lty = c(NA, NA, 1, 1),
         lwd = c(NA, NA, 2.5, 1.5),
         cex = 0.8, bty = "n")
}


plot_rda_triplot(rda_result, 
                 axes = c(1, 2),
                 arrow_col_sites = "steelblue",
                 arrow_col_env = "red3",
                 arrow_col_spe = "forestgreen",
                 mult_env = 2,
                 mult_spe = 1.5  )
```

```


#now calculate the contributions by hand
contrib1=100*pcaivdoubs$cw*pcaivdoubs$co[,1]^2/pcaivdoubs$eig[1]
contrib2=100*pcaivdoubs$cw*pcaivdoubs$co[,2]^2/pcaivdoubs$eig[2]
contrib1
contrib2

#ask for column absolute contributions
inertia.dudi(pcaivdoubs, col = TRUE, row = TRUE)


#find the canonical coefficient (i.e.the equivalent of regression coefficients for each explanatory variable on each canonical axis)
coef(rdadoubs)



#Choice of linear (RDA) or unimodal (CCA) model ?
#perform a DCA to decide whether the RDA or the CCA is best suited to the data set:
#The first coinertia is also better because it uses the weights of the CA for the fish data, which gives more importance to the stations with more species.


# Interpretation  neet to be done | NMDS ?
one the same graphe : Env et bio pour tirer des conclusions avec go
```{r}
goodness(rda3)
```

